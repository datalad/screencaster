#!/usr/bin/python
#emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
#ex: set sts=4 ts=4 sw=4 noet:
#------------------------- =+- Python script -+= -------------------------
"""
 @file      fetch_url.py
 @date      Wed May 22 15:31:19 2013
 @brief


  Yaroslav Halchenko                                            Dartmouth
  web:     http://www.onerussian.com                              College
  e-mail:  yoh@onerussian.com                              ICQ#: 60653192

 DESCRIPTION (NOTES):

 COPYRIGHT: Yaroslav Halchenko 2013

 LICENSE: MIT

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to deal
  in the Software without restriction, including without limitation the rights
  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
  copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
  THE SOFTWARE.
"""
#-----------------\____________________________________/------------------

__author__ = 'Yaroslav Halchenko'
__copyright__ = 'Copyright (c) 2013 Yaroslav Halchenko'
__license__ = 'MIT'
__version__ = "0.0.0.dev"

import calendar
import commands
import email.utils
import json
import git
import gzip
import os
import re
import shutil
import time
import urllib2

from urlparse import urljoin, urlsplit
from StringIO import StringIO

from ConfigParser import ConfigParser

from BeautifulSoup import BeautifulSoup



# All this should be replaced with a use of patoolib, but
# after figure out why
# https://github.com/wummel/patool/issues/2
# Fixed in 1.2 release, but there is no __version__ to assess reliably (yet)
import patoolib

DECOMPRESSORS = {
    '\.(tar\.bz|tbz)$' : 'tar -xjvf %(file)s -C %(dir)s',
    '\.(tar\.xz)$' : 'tar -xJvf %(file)s -C %(dir)s',
    '\.(tar\.gz|tgz)$' : 'tar -xzvf %(file)s -C %(dir)s',
    '\.(zip)$' : 'unzip %(file)s -d %(dir)s',
    }


def decompress_file(file, dir):
    fullcmd = None
    for ptr, cmd in DECOMPRESSORS.iteritems():
        if re.search(ptr, file):
            fullcmd = cmd % locals()
            break
    if fullcmd is not None:
        lgr.debug("Extracting file: %s" % fullcmd)
        status, output = getstatusoutput(fullcmd)
        if status:
            lgr.debug("Failed to extract: status %d output %s" % (status, output))
    else:
        lgr.debug("Have no clue how to extract %s -- using patool" % file)
        patoolib.extract_archive(file, outdir=dir)
    # TODO: either remove leading directory,
    #       or may be remove containing directory


## url = 'http://human.brain-map.org/api/v2/well_known_file_download/178238387'
## urlz = 'http://gzip.datagit.org/zeros100'
## urlzgz = 'http://gzip.datagit.org/zeros100.gz'

def dry(s, dry):
    if dry:
        return "DRY " + s
    return s


def getstatusoutput(cmd, dry_run=False):
    lgr.debug(dry("Running: %s" % (cmd,), dry_run))
    if not dry_run:
        status, output = commands.getstatusoutput(cmd)
        if status != 0:
            msg = "Failed to run %r. Exit code=%d output=%s" % (cmd, status, output)
            lgr.error(msg)
            raise RuntimeError(msg)
        else:
            return status, output
    return None, None


def get_response_filename(url, response_info):
    if 'Content-Disposition' in response_info:
        # If the response has Content-Disposition, try to get filename from it
        cd = dict(map(
            lambda x: x.strip().split('=') if '=' in x else (x.strip(),''),
            response_info['Content-Disposition'].split(';')))
        if 'filename' in cd:
            filename = cd['filename'].strip("\"'")
            return filename
    return None


def get_response_stamp(response_info):
    size, mtime = None, None
    if 'Content-length' in response_info:
        size = int(response_info['Content-length'])
    if 'Last-modified' in response_info:
        mtime = calendar.timegm(email.utils.parsedate(
            response_info['Last-modified']))
    return dict(size=size, mtime=mtime)


def __download(url, filename=None, filename_only=False):
    # http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way
    request = urllib2.Request(url)
    request.add_header('Accept-encoding', 'gzip,deflate')
    r = urllib2.urlopen(request)
    try:
        filename = filename or getFileName(url, r)
        if not filename_only:
            with open(filename, 'wb') as f:
                if r.info().get('Content-Encoding') == 'gzip':
                    buf = StringIO( r.read())
                    src = gzip.GzipFile(fileobj=buf)
                else:
                    src = r
                shutil.copyfileobj(src, f)
    finally:
        r.close()
    return filename


class RegexpType(object):
    """Factory for creating regular expression types for argparse

    DEPRECATED AFAIK -- now things are in the config file...
    but we might provide a mode where we operate solely from cmdline
    """
    def __call__(self, string):
        if string:
            return re.compile(string)
        else:
            return None


def fetch_page(url):
    lgr.debug("Fetching %s" % url)
    page = urllib2.urlopen(url).read()
    lgr.info("Fetched %d bytes page from %s" % (len(page), url))
    return page


def parse_urls(page):
    lgr.debug("Parsing out urls")
    soup = BeautifulSoup(page)
    return [(link.get('href'), link.text)
            for link in soup.findAll('a')]


def filter_urls(urls,
                include_href=None,
                exclude_href=None,
                include_href_a=None,
                exclude_href_a=None):

    if (not include_href) and not (include_href_a):
        include_href = '.*'               # include all

    # First do all includes explicitly and then excludes
    return [(url, a)
             for url, a in urls
                if url
                   and
                   ((include_href and re.search(include_href, url))
                     or (include_href_a and re.search(include_href_a, a))) \
                   and not
                   ((exclude_href and re.search(exclude_href, url))
                     or (exclude_href_a and re.search(exclude_href_a, a)))]


def download_url(href, incoming, url_stamps=None, dry_run=False, fast_mode=False):

    updated = False
    # so we could check and remove it to keep it clean
    temp_full_filename = None

    if url_stamps is None:
        url_stamps = {}

    url_filename = os.path.basename(urlsplit(href).path)

    class ReturnSooner(Exception):
        pass

    try: # might RF -- this is just to not repeat the same return
        if dry_run:
            # we can only try to deduce from the href...
            filename = url_filename
            full_filename = os.path.join(incoming, filename)
            # and not really do much
            lgr.debug("Nothing else could be done for download in dry mode")
            raise ReturnSooner

        # http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way
        request = urllib2.Request(href)

        # No traffic compression since we do not know how to identify
        # exactly either it has to be decompressed
        # request.add_header('Accept-encoding', 'gzip,deflate')
        r = urllib2.urlopen(request)
        try:
            r_info = r.info()

            r_stamp = get_response_stamp(r_info)
            filename = get_response_filename(href, r_info) or url_filename
            full_filename = os.path.join(incoming, filename)
            if r_stamp['size']:
                lgr.debug("File %s is of size %d" % (filename, r_stamp['size']))

            if url_filename != filename:
                lgr.debug("Filename in url %s differs from the load %s" % (url_filename, filename))

            # So we have filename -- time to figure out either we need to re-download it

            # url_stamps might maintain information even if file is not present, e.g.
            # if originally we haven't kept originals
            download = False

            def _compare_stamps(ofs, nfs, msg):
                """ofs -- old stamps, nfs -- new ones
                """
                download = False
                for k in ofs.keys():
                    n, o = nfs[k], ofs[k]
                    if n:
                        if o != n:
                            lgr.debug("Response %s %s differs from %s %s -- download"
                                      % (k, n, msg, o))
                            download = True
                return download

            if full_filename in url_stamps:
                # if it is listed and has no stamps?
                # no -- it should have stamps -- we will place them there if none
                # was provided in HTML response. and we will redownload only
                # if any of those returned in HTTP header non-None and differ.
                # Otherwise we would assume that we have it
                # TODO: think if it is not too fragile
                download |= _compare_stamps(url_stamps.get(full_filename),
                                            r_stamp, "previous")

            if os.path.exists(full_filename):
                lgr.debug("File %s already exists under %s" % (filename, full_filename))

                # Verify time stamps etc
                file_stat = os.stat(full_filename)
                ex_stamp = dict(size=file_stat.st_size,
                                    mtime=file_stat.st_mtime)

                download |= _compare_stamps(ex_stamp, r_stamp, "file stats")

            if not (full_filename in url_stamps) and not os.path.exists(full_filename):
                lgr.debug("File %s is not known and doesn't exist" % filename)
                download = True

            if fast_mode:
                lgr.info("Not downloading -- fast mode")
                raise ReturnSooner

            if not download:
                raise ReturnSooner

            lgr.info("Need to download file %s into %s" % (filename, full_filename))

            if os.path.exists(full_filename):
                lgr.debug("Removing previously existing file")
                # TODO

            # actual download -- quite plain -- may be worth to offload to
            # wget or curl for now?
            temp_full_filename = full_filename + '.download'

            if os.path.exists(temp_full_filename):
                raise RuntimeError("File %s should not be there yet" % temp_full_filename)

            try:
                # we might need the directory
                full_filename_dir = os.path.dirname(temp_full_filename)
                if not os.path.exists(full_filename_dir):
                    os.makedirs(full_filename_dir)

                with open(temp_full_filename, 'wb') as f:
                    # No magical decompression for now
                    if False: #r.info().get('Content-Encoding') == 'gzip':
                        buf = StringIO( r.read())
                        src = gzip.GzipFile(fileobj=buf)
                    else:
                        src = r
                    shutil.copyfileobj(src, f)
            except Exception, e:
                lgr.error("Failed to download: %s" % e)
                if os.path.exists(temp_full_filename):
                    lgr.info("Removing %s" % temp_full_filename)
                    os.unlink(temp_full_filename)
                raise

            mtime = r_stamp['mtime']
            size = r_stamp['size']

            if mtime:
                lgr.debug("Setting downloaded file's mtime to %s obtained from HTTP header"
                          % r_stamp['mtime'])
                os.utime(temp_full_filename, (time.time(), mtime))

            # Get stats and check on success
            # TODO: may be some would have MD5SUMS associated?
            updated = True

            # get mtime so we could update entry for our file
            file_stat = os.stat(temp_full_filename)
            new_mtime = file_stat.st_mtime
            new_size = file_stat.st_size

            if mtime and (new_mtime != mtime):
                lgr.debug("Set mtime differs for some reason.  Got %s (%s) while it should have been %s (%s)"
                          % (new_mtime, time.gmtime(new_mtime),
                             mtime, time.gmtime(mtime)))
                updated = False

            if size and (new_size != size):
                lgr.debug("Downloaded file differs in size.  Got %d while it should have been %d"
                          % (new_size, size))
                updated = False

            if updated:
                # TODO: we might like to git annex drop previously existed file etc
                os.rename(temp_full_filename, full_filename)

                url_stamps[full_filename] = dict(mtime=mtime, size=size)
            else:
                pass
        finally:
            r.close()
            if temp_full_filename and os.path.exists(temp_full_filename):
                lgr.debug("Removing left-over %s of size %d"
                          % (temp_full_filename, os.stat(temp_full_filename).st_size))
                os.unlink(temp_full_filename)

    except ReturnSooner:
        # We have handled things already, just need to return
        pass

    return filename, full_filename, updated


class AnnexRepo(object):

    def __init__(self, path, dry_run=False):
        self.path = path
        self.dry_run = dry_run

    def run(self, cmd):
        cmdfull = "cd %s && git annex %s" % (self.path, cmd)
        return getstatusoutput(cmdfull, self.dry_run)

    def dry(self, s):
        return dry(s, self.dry_run)

    def init(self,  description=""):

        lgr.info(self.dry(
            "Initializing git annex repository under %s: %s"
            % (self.path, description)))

        status, output = getstatusoutput(
            "cd %s && git init && git annex init" % self.path, self.dry_run)
        lgr.debug("Successfully initialized")

        if not self.dry_run:
            # dump description
            with open(os.path.join(self.path, '.git', 'description'), 'w') as f:
                f.write(description + '\n')


    def add_file(self, annex_filename, href=None, add_mode='auto',
                       annex_opts=""):
        """
        If add_mode=='auto' we assume that if file doesn't exist already --
        it should be '--fast' added
        """
        # We delay actual committing to git-annex until later
        annex_opts = annex_opts + ' -c annex.alwayscommit=false'
        if add_mode == 'auto':
            if os.path.exists(annex_filename):
                add_mode = "download"
            else:
                if href is None:
                    raise ValueError("No file and no href -- can't add to annex")
                fast_mode = "fast"

        if href:
            annex_cmd = "addurl %s --file %s %s %s" \
              % (annex_opts, os.path.basename(annex_filename),
                 {'download': '', 'fast': '--fast'}[add_mode],
                 href)
        else:
            annex_cmd = "add %s %s" % (annex_opts, os.path.basename(annex_filename),)

        return self.run(annex_cmd)


def annex_file(href,
               incoming_filename, annex_filename,
               incoming_annex, public_annex,
               archives_destiny=None,
               archives_re=None,
               add_mode='auto',
               uncomp_strip_leading_dir=True, #  False; would strip only if 1 directory
               addurl_opts=None,
               dry_run=False
               ):
    lgr.info("Annexing %s originating from url=%s present locally under %s"
             % (annex_filename, href, incoming_filename))

    # it might be that we would like to move it
    # or extract it
    is_archive = False
    if archives_re:
        res = re.search(archives_re, annex_filename)
        if res:
            is_archive = True
            annex_dir = annex_filename[:res.start()]
            if res.end() < len(res.string):
                annex_dir += annex_filename[res.end:]
            annex_filename = annex_dir   # TODO: "merge" the two

    if is_archive:
        #import pydb; pydb.debugger()
        # what if the directory exist already?
        # option? yeah -- for now we will just REMOVE and recreate with new files
        temp_annex_dir = annex_dir + ".extract"
        os.makedirs(temp_annex_dir)
        decompress_file(incoming_filename, temp_annex_dir)
        if os.path.exists(annex_dir):
            lgr.debug("Removing previously present %s" % annex_dir)
            shutil.rmtree(annex_dir)
        os.rename(temp_annex_dir, annex_dir)
        # TODO: some files might need to go to GIT directly
        public_annex.add_file(annex_dir)

        # what do we do with the "incoming" archive
        if archives_destiny == 'rm':
            shutil.rmtree(incoming_filename, True)
        elif archives_destiny in ('annex', 'drop'):
            incoming_annex.add_file(incoming_filename, href=href)
            if archives_destiny == 'drop':
                incoming_annex.run("drop %s" % annex_filename)
        elif archives_destiny == 'keep':
            pass # do nothing more
        else:
            raise ValueError("Unknown value of archives_destiny=%r"
                             % archives_destiny)
    else:
        # Figure out if anything needs to be done to it
        # TODO: some files might need to go to GIT directly
        incoming_annex.add_file(incoming_filename, href=href, add_mode=add_mode)
        # copy via linking (TODO -- option to move, copy?)

        if incoming_annex is not public_annex:
            if os.path.exists(incoming_filename):
                os.link(incoming_filename, annex_filename)
            else:
                # assuming --fast mode
                pass
            pubic_annex.add_file(annex_filename, href=href, add_mode=add_mode)


def git_commit(path, files=None, msg=None, dry_run=False):
    if msg is None:
        msg = 'page2annex: Committing staged changes as of %s' \
               % time.strftime('%Y/%m/%d %H:%M:%S')

    repo = git.Repo(path)

    if files: # smth to add to commit?
        repo.index.add(files)

    # anything staged to be committed
    # it might be invalid at the very beginning ;)
    if not repo.head.is_valid() or len(repo.index.diff(repo.head.commit)):
        #repo.git.commit(m=msg, a=True)
        repo.index.commit(msg)
        repo.index.update()
        assert(not len(repo.index.diff(repo.head.commit)))
        # cmd = "cd %s; git commit -m %r" % (path, msg)
        # status, output = getstatusoutput(cmd, dry_run)

def pprint_indent(l, indent="", fmt='%s'):
    return indent + ('\n%s' % indent).join([fmt % x for x in l])

# Some rudimentary tests
from nose.tools import assert_equal, assert_raises, assert_greater, raises, \
    make_decorator, ok_, eq_
import tempfile
tempfile.template = 'tmp-page2annex'

import stat, os
def rmtree(path, *args, **kwargs):
    """To remove git-annex .git it is needed to make all files and directories writable again first
    """
    for root, dirs, files in os.walk(path):
        for f in files:
            fullf = os.path.join(root, f)
            os.chmod(fullf, os.stat(fullf).st_mode | stat.S_IWRITE | stat.S_IREAD)
        os.chmod(root, os.stat(root).st_mode | stat.S_IWRITE | stat.S_IREAD)
    shutil.rmtree(path, *args, **kwargs)

def _create_tree(path, tree):
    """Given a list of tuples (name, load) create such a tree

    if load is a tuple itself -- that would create either a subtree or an archive
    with that content and place it into the tree if name ends with .tar.gz
    """
    if not os.path.exists(path):
        os.makedirs(path)

    for name, load in tree:
        full_name = os.path.join(path, name)
        if isinstance(load, tuple):
            if name.endswith('.tar.gz'):
                dirname = name[:-7]
                full_dirname = os.path.join(path, dirname)
                os.makedirs(full_dirname)
                _create_tree(full_dirname, load)
                # create archive
                status, output = getstatusoutput(
                    'cd %(path)s && tar -czvf %(name)s %(dirname)s' % locals())
                # remove original tree
                shutil.rmtree(full_dirname)
            else:
                _create_tree(full_name, load)
        else:
            with open(full_name, 'w') as f:
                f.write(load)

def with_tree(tree, **tkwargs):
    def decorate(func):
        def newfunc(*arg, **kw):
            d = tempfile.mkdtemp(**tkwargs)
            _create_tree(d, tree)
            try:
                func(*(arg + (d,)), **kw)
            finally:
                #print "TODO: REMOVE tree ", d
                shutil.rmtree(d)

        newfunc = make_decorator(func)(newfunc)
        return newfunc
    return decorate


# GRRR -- this one is crippled since path where HTTPServer is serving
# from can't be changed without pain. 

import SimpleHTTPServer
import SocketServer
from threading import Thread

def serve_path_via_http(*args, **tkwargs):
    def decorate(func):
        def newfunc(path, *arg, **kw):
            port = 8006
            print "Starting to serve path ", path
            # TODO: ATM we are relying on path being local so we could
            # start HTTP server in the same directory.  FIX IT!
            httpd = SocketServer.TCPServer(("", port),
                        SimpleHTTPServer.SimpleHTTPRequestHandler)
            server_thread = Thread(target=httpd.serve_forever)
            server_thread.start()
            #time.sleep(1)               # just give it few ticks
            try:
                func(*(('http://localhost:%d/%s/' % (port, path),)+arg), **kw)
            finally:
                #print "Stopping server"
                httpd.shutdown()
                #print "Waiting for thread to stop"
                server_thread.join()
        newfunc = make_decorator(func)(newfunc)
        return newfunc
    return decorate


def test_filter_urls():
    urls = [('/x.nii.gz', 'bogus'),
            ('x.tar.gz', None),
            ('y', None)]
    eq_(filter_urls(urls, "^x\..*"), [urls[1]])
    eq_(filter_urls(urls, "^[xy]"), urls[1:])
    eq_(filter_urls(urls, "\.gz", "\.nii"),
           [urls[1]])
    eq_(filter_urls(urls, exclude_href="x"),
           [urls[2]])
    eq_(filter_urls(urls, "^[xy]"), urls[1:])

def test_get_response_stamp():
    r = get_response_stamp({'Content-length': '101',
                                'Last-modified': 'Wed, 01 May 2013 03:02:00 GMT'})
    eq_(r['size'], 101)
    eq_(r['mtime'], 1367377320)


def test_download_url():
    # let's simulate the whole scenario
    fd, fname = tempfile.mkstemp()
    dout = fname + '-d'
    # TODO move tempfile/tempdir setup/cleanup into fixture(s)
    os.mkdir(dout)
    os.write(fd, "How do I know what to say?\n")
    os.close(fd)

    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    ok_(updated)
    # check if stats are the same
    s, s_ = os.stat(fname), os.stat(full_filename)
    eq_(s.st_size, s_.st_size)
    # at least to a second
    eq_(int(s.st_mtime), int(s_.st_mtime))

    # and if again -- should not be updated
    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    ok_(not updated)

    # but it should if we remove it
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    ok_(updated)
    # check if stats are the same
    s_ = os.stat(full_filename)
    eq_(s.st_size, s_.st_size)
    eq_(int(s.st_mtime), int(s_.st_mtime))

    # and what if we maintain url_stamps
    url_stamps = {}
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout, url_stamps)
    ok_(updated)
    ok_(full_filename in url_stamps)
    s_ = os.stat(full_filename)
    eq_(int(s.st_mtime), int(s_.st_mtime))
    eq_(int(s.st_mtime), url_stamps[full_filename]['mtime'])
    print filename, full_filename, url_stamps,

    # and if we remove it but maintain information that we think it
    # exists -- we should skip it ATM
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout, url_stamps)
    ok_(not updated)
    ok_(full_filename in url_stamps)
    assert_raises(OSError, os.stat, full_filename)

    os.unlink(fname)
    rmtree(dout, True)


@with_tree((('test.txt', 'abracadabra'),
            ('1.tar.gz', (
               ('1f.txt', '1f load'),
               ('d', (('1d', ''),
                      )),
               ))),
           dir=os.curdir,
           prefix='.tmp-page2annex-')
@serve_path_via_http()
def test_parse_urls_recurse(url):
    dout = tempfile.mkdtemp()
    page = fetch_page(url)
    urls = parse_urls(page)

    cfg = get_default_config(dict(
        DEFAULT=dict(
            incoming=dout,
            public=dout,
            description="test",
            ),
        files=dict(
            directory='files', # TODO: recall what was wrong with __name__ substitution, look into fail2ban/client/configparserinc.py
            url=url)))

    stats1 = rock_and_roll(cfg, dry_run=False)
    assert_greater(stats1['annex_updates'], 0)   # TODO: more specific
    eq_(stats1['downloads'], 2)

    # Let's repeat -- there should be no downloads/updates
    stats2 = rock_and_roll(cfg, dry_run=False)
    eq_(stats2['annex_updates'], 0)
    eq_(stats2['downloads'], 0)

    # TODO: verify that the structure as it should be

    #print dout, "URLS: ", urls
    rmtree(dout, True)
    #print dout

#
# DB
#
def load_db(path):
    with open(path) as f:
        return json.load(f)

def save_db(db, path):
    with open(path, 'w') as f:
        json.dump(db, f, indent=2)


#
# Configuration
#
def get_default_config(sections={}):
    cfg = ConfigParser(defaults=dict(
        mode='download',                 # TODO -- check, use
        orig='auto',                 # TODO -- now we don't use it
        meta_info='True',                 # TODO -- now we just use it
        directory="%(__name__)s",
        archives_re='(%s)' % ('|'.join(DECOMPRESSORS.keys())),
        # 'keep'  -- just keep original without annexing it etc
        # 'annex' -- git annex addurl ...
        # 'drop'  -- 'annex' and then 'annex drop' upon extract
        # 'rm'    -- remove upon extraction
        # 'auto':
        # if incoming == public:
        #  'auto' == 'rm'
        # else:
        #  'auto' == 'annex'
        archives_destiny="auto",
        incoming="repos/incoming/EXAMPLE",
        public="repos/public/EXAMPLE",
        include_href='',
        include_href_a='',
        exclude_href='',
        exclude_href_a='',
        filename='&(filename)s',
        # Checks!
        check_url_limit='0',                     # no limits
        # unused... we might like to enable it indeed and then be
        # smart with our actions in extracting archives into
        # directories which might contain those files, so some might
        # need to be annexed and some directly into .git
        # TODO
        #annex='.*',                    # 
        ))
    for section, options in sections.iteritems():
        if section != 'DEFAULT':
            cfg.add_section(section)
        for opt, value in options.iteritems():
            cfg.set(section, opt, value)
    return cfg

def load_config(configs):
    # Load configuration
    cfg = get_default_config()
    cfg_read = cfg.read(configs)
    assert(cfg_read == configs)
    return cfg

#
# Main loop
#
def rock_and_roll(cfg, dry_run=False, db_name = '.page2annex'):
    """Given a configuration fetch/update git-annex "clone"
    """

    # Let's output summary stats at the end
    stats = dict([(k, 0) for k in
                  ['sections', 'urls', 'downloads', 'annex_updates', 'size']])
    pages_cache = {}

    dry_str = "DRY: " if dry_run else ""

    incoming = cfg.get('DEFAULT', 'incoming')
    public = cfg.get('DEFAULT', 'public')

    #
    # Initializing file structure
    #
    if not (os.path.exists(incoming) and os.path.exists(public)):
        lgr.debug("%sCreating directories for incoming (%s) and public (%s) annexes"
                  % (dry_str, incoming, public))

        if not dry_run:
            if not os.path.exists(incoming):
                os.makedirs(incoming)
            if not os.path.exists(public):
                os.makedirs(public)           #TODO might be the same

    if not os.path.exists(os.path.join(public, '.git', 'annex')):
        public_annex = AnnexRepo(public, dry_run=dry_run)
        description = cfg.get('DEFAULT', 'description')
        public_annex.init(description)
        if public != incoming:
            incoming_annex = AnnexRepo(incoming, dry_run=dry_run)
            incoming_annex.init(description + ' (incoming)')
        else:
            incoming_annex = public_annex

    # TODO: load previous status info
    """We need

    url_stamps -- to track their time.  URLs might or might not provide Last-Modified,
      so if not provided, would correspond to None and only look by url change pretty much

    annex_pairs -- to have clear correspondence between annex_filename and url.
                   annex_filename might correspond to a directory where we would
                   extract things, so we can't just geturl on it
    """

    db_path = os.path.join(incoming, db_name)
    if os.path.exists(db_path):
        status_info = load_db(db_path)
    else:
        # create fresh
        status_info = dict(url_stamps={},   # url -> (mtime, size (AKA Content-Length, os.stat().st_size ))
                           annex_pairs={})      # annex_filename -> url

    url_stamps = status_info['url_stamps']
    annex_pairs = status_info['annex_pairs']

    # TODO: look what is in incoming for this "repository", so if
    # some urls are gone or changed so previous file is not there
    # we would clean-up upon exit

    # each section defines a separate download setup
    for section in cfg.sections():
        lgr.info("Section: %s" % section)
        stats['sections'] += 1

        # some checks
        add_mode = cfg.get(section, 'mode')
        assert(add_mode in ['download', 'fast'])

        section_dir = cfg.get(section, 'directory')

        incoming_section = os.path.join(incoming, section_dir)
        public_section = os.path.join(public, section_dir)

        if not (os.path.exists(incoming) and os.path.exists(public)):
            lgr.debug("%sCreating directories for section's incoming (%s) and public (%s) annexes"
                      % (dry_str, incoming_section, public_section))
            if not dry_run:
                os.makedirs(incoming_section)
                os.makedirs(public_section)           #TODO might be the same

        scfg = dict(cfg.items(section))

        archives_destiny = scfg.get('archives_destiny')
        if archives_destiny == 'auto':
            archives_destiny = 'rm' if incoming == public else 'annex'

        # Fetching the page (possibly again! TODO: cache)
        url = scfg['url']
        page = pages_cache.get(url, None) or fetch_page(url)
        pages_cache[url] = page

        #
        # Parse out all URLs, as a tuple (url, a(text))
        urls_all = parse_urls(page)
        lgr.info("Got total %d urls" % len(urls_all))
        #lgr.debug("%d urls:\n%s" % (len(urls_all), pprint_indent(urls_all, "    ", "[%s](%s)")))

        # Filter them out
        urls = filter_urls(urls_all, **dict(
            [(k,scfg[k]) for k in
             ('include_href', 'exclude_href', 'include_href_a', 'exclude_href_a')]))
        lgr.info("%d out of %d urls survived filtering" % (len(urls), len(urls_all)))
        if len(set(urls)) < len(urls):
            urls = sorted(set(urls))
            lgr.info("%d unique urls" % (len(urls),))
        lgr.debug("%d urls:\n%s" % (len(urls), pprint_indent(urls, "    ", "[%s](%s)"))) 
        if scfg.get('check_url_limit', None):
            limit = int(scfg['check_url_limit'])
            if limit:
                if len(urls) > limit:
                    raise RuntimeError("Cannot process section since we expected only %d urls"
                                       % limit)

        #
        # Process urls
        for href, href_a in urls:
            # bring them into the full urls
            href = urljoin(scfg['url'], href)
            lgr.debug("Working on [%s](%s)" % (href, href_a))

            # Will adjust url_stamps in-place
            filename, full_filename, href_updated = \
              download_url(href, incoming_section, url_stamps=url_stamps, dry_run=dry_run,
                           fast_mode=add_mode=='fast')

            if href_updated:
                stats['downloads'] += 1
                stats['size'] += os.stat(full_filename).st_size
                if not dry_run:
                    save_db(status_info, db_path)
                pass

            # figure out what should it be
            annex_filename = scfg['filename'].replace('&', '%') % locals()   # interpolate
            annex_full_filename = os.path.join(public_section, annex_filename)

            annex_updated = False
            if href_updated or (not annex_filename in annex_pairs):

                # TODO: here figure it out either it will be a
                # directory or not and either it needs to be extracted,
                # and what will be the extracted directory name

                # Place them under git-annex, if they do not exist already
                annex_file(
                    href,
                    incoming_filename=full_filename,
                    annex_filename=annex_full_filename,
                    incoming_annex=incoming_annex,
                    public_annex=public_annex,
                    archives_destiny=archives_destiny,
                    archives_re=scfg.get('archives_re'),
                    add_mode=add_mode,
                    addurl_opts=scfg.get('addurl_opts', None),
                    dry_run=dry_run,
                    )

                annex_pairs[annex_filename] = href
                annex_updated = True
                stats['annex_updates'] += 1
            else:
                # TODO: shouldn't we actually check???
                lgr.debug("Skipping annexing %s since it must be there already (TODO)" % annex_filename)

            if not dry_run and (annex_updated or href_updated):
                save_db(status_info, db_path)

            stats['urls'] += 1

    git_commit(incoming, files=[db_name])
    if incoming != public:
        git_commit(public)

    lgr.info("Processed %(sections)d sections, %(urls)d urls, "
             "%(downloads)d downloads with %(size)d bytes. Made %(annex_updates)s git/annex additions/updates" % stats)
    return stats


if __name__ == '__main__':

    # setup cmdline args parser
    # main parser
    import argparse
    import logging
    import os
    import sys

    lgr = logging.getLogger('page2annex')

    parser = argparse.ArgumentParser(
        fromfile_prefix_chars='@',
        description="""
    Fetch web page's linked content into git-annex repository.

    """,
        epilog='',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        add_help=True,
        version=__version__
    )

    # common options
#    parser.add_argument(
#        "-a", "--addurl-opts", default="",
#        help="Additional options to pass to 'git annex addurl', e.g. --fast")
#    parser.add_argument(
#        "-i", "--include", type=RegexpType(),
#        help="Include links which match regular expression (by HREF). "
#             "Otherwise all are considered")
#    parser.add_argument(
#        "-e", "--exclude", type=RegexpType(),
#        help="Exclude links which match regular expression (by HREF)")
#     parser.add_argument(
#         "--use-a", action="store_true",
#         help="Use name provided in the link, not filename given by the server")

    # TODO: we might want few modes than just dry:
    #   act [default] as now without dry-run
    #   monitor -- the one like dry BUT which would collect all "interesting"
    #              entries and if anything to be actually done -- spit them
    #              out or email?
    parser.add_argument(
        "-n", "--dry-run", action="store_true",
        help="No git-annex is invoked, commands are only printed to the screen")

    parser.add_argument(
        "--tests-only", action="store_true",
        help="Do not do anything but run tests")

    parser.add_argument(
        '-l', '--log-level',
        choices=('critical', 'error', 'warning', 'info', 'debug'),
        default='warning',
        help="""level of verbosity""")

    if __debug__:
        # borrowed from Michael's bigmess
        parser.add_argument(
            '--dbg', action='store_true', dest='common_debug',
            help="do not catch exceptions and show exception traceback")


    parser.add_argument("configs", metavar='file', nargs='+',
                        help="Configuration file(s) defining the structure of the 'project'")

    args = parser.parse_args() #['-n', '-l', 'debug', 'allen-genetic.cfg'])

    # Basic logging setup
    console = logging.StreamHandler(sys.stdout)
    console.setFormatter(logging.Formatter("%(levelname)-6s %(message)s"))
    lgr.addHandler(console)
    lgr.setLevel(getattr(logging, args.log_level.upper()))

    lgr.debug("Command line arguments: %r" % args)

    lgr.info("Running tests")
    test_parse_urls_recurse()
    test_filter_urls()
    test_get_response_stamp()
    test_download_url()
    if args.tests_only:
        raise SystemExit(0)

    try:
        lgr.info("Reading configs")
        cfg = load_config(args.configs)
        rock_and_roll(cfg, args.dry_run)

    except Exception as exc:
        lgr.error('%s (%s)' % (str(exc), exc.__class__.__name__))
        if __debug__ and args.common_debug:
            import pdb
            pdb.post_mortem()
        raise

