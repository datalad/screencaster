#!/usr/bin/python
#emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
#ex: set sts=4 ts=4 sw=4 noet:
#------------------------- =+- Python script -+= -------------------------
"""
 @file      fetch_url.py
 @date      Wed May 22 15:31:19 2013
 @brief


  Yaroslav Halchenko                                            Dartmouth
  web:     http://www.onerussian.com                              College
  e-mail:  yoh@onerussian.com                              ICQ#: 60653192

 DESCRIPTION (NOTES):

 COPYRIGHT: Yaroslav Halchenko 2013

 LICENSE: MIT

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to deal
  in the Software without restriction, including without limitation the rights
  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
  copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
  THE SOFTWARE.
"""
#-----------------\____________________________________/------------------

__author__ = 'Yaroslav Halchenko'
__copyright__ = 'Copyright (c) 2013 Yaroslav Halchenko'
__license__ = 'MIT'
__version__ = "0.0.0.dev"

import calendar
import commands
import email.utils
import gzip
import os
import re
import shutil
import time
import urllib2

from collections import namedtuple

from urlparse import urljoin, urlsplit
from StringIO import StringIO

from ConfigParser import ConfigParser

from BeautifulSoup import BeautifulSoup



# All this should be replaced with a use of patoolib, but
# after figure out why
# https://github.com/wummel/patool/issues/2
import patoolib

DECOMPRESSORS = {
    '(tar.bz|tbz)' : 'tar -xjvf %(file)s -C %(dir)s',
    '(tar.xz)' : 'tar -xJvf %(file)s -C %(dir)s',
    '(tar.gz|tgz)' : 'tar -xzvf %(file)s -C %(dir)s',
    '(zip)' : 'unzip %(file)s -d %(dir)s',
    }

def decompress_file(file, dir):
    fullcmd = None
    for ptr, cmd in DECOMPRESSORS.iteritems():
        if re.match('.*%s$' % ptr, file):
            fullcmd = cmd % locals()
            break
    if fullcmd is not None:
        #lgr.debug("Extracting file: %s" % fullcmd)
        status, output = getstatusoutput(fullcmd)
    else:
        lgr.debug("Have no clue how to extract %s -- using patool" % file)
        patoolib.extract_archive(file, outdir=dir)


## url = 'http://human.brain-map.org/api/v2/well_known_file_download/178238387'
## urlz = 'http://gzip.datagit.org/zeros100'
## urlzgz = 'http://gzip.datagit.org/zeros100.gz'

def dry(s, dry):
    if dry:
        return "DRY " + s
    return s

def getstatusoutput(cmd, dry_run=False):
    lgr.debug(dry("Running: %s" % (cmd,), dry_run))
    if not dry_run:
        status, output = commands.getstatusoutput(cmd)
        if status != 0:
            msg = "Failed to run %r. Exit code=%d output=%s" % (cmd, status, output)
            lgr.error(msg)
            raise RuntimeError(msg)
        else:
            return status, output
    return None, None


def get_response_filename(url, response_info):
    if 'Content-Disposition' in response_info:
        # If the response has Content-Disposition, try to get filename from it
        cd = dict(map(
            lambda x: x.strip().split('=') if '=' in x else (x.strip(),''),
            response_info['Content-Disposition'].split(';')))
        if 'filename' in cd:
            filename = cd['filename'].strip("\"'")
            return filename
    return None

def get_response_filestamp(response_info):
    size, mtime = None, None
    if 'Content-length' in response_info:
        size = int(response_info['Content-length'])
    if 'Last-modified' in response_info:
        mtime = calendar.timegm(email.utils.parsedate(
            response_info['Last-modified']))
    return dict(size=size, mtime=mtime)

def __download(url, filename=None, filename_only=False):
    # http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way
    request = urllib2.Request(url)
    request.add_header('Accept-encoding', 'gzip,deflate')
    r = urllib2.urlopen(request)
    try:
        filename = filename or getFileName(url, r)
        if not filename_only:
            with open(filename, 'wb') as f:
                if r.info().get('Content-Encoding') == 'gzip':
                    buf = StringIO( r.read())
                    src = gzip.GzipFile(fileobj=buf)
                else:
                    src = r
                shutil.copyfileobj(src, f)
    finally:
        r.close()
    return filename


class RegexpType(object):
    """Factory for creating regular expression types for argparse

    DEPRECATED AFAIK -- now things are in the config file...
    but we might provide a mode where we operate solely from cmdline
    """
    def __call__(self, string):
        if string:
            return re.compile(string)
        else:
            return None

def fetch_page(url):
    lgr.debug("Fetching %s" % url)
    page = urllib2.urlopen(url).read()
    lgr.info("Fetched %d bytes page from %s" % (len(page), url))
    return page

def parse_urls(page):
    lgr.debug("Parsing out urls")
    soup = BeautifulSoup(page)
    return [(link.get('href'), link.text)
            for link in soup.findAll('a')]

def filter_urls(urls,
                include_href=None,
                exclude_href=None,
                include_href_a=None,
                exclude_href_a=None):

    if (not include_href) and not (include_href_a):
        include_href = '.*'               # include all

    # First do all includes explicitly and then excludes
    return [(url, a)
             for url, a in urls
                if url
                   and
                   ((include_href and re.search(include_href, url))
                     or (include_href_a and re.search(include_href_a, a))) \
                   and not
                   ((exclude_href and re.search(exclude_href, url))
                     or (exclude_href_a and re.search(exclude_href_a, a)))]


def download_url(href, incoming, url_filestamps=None, dry_run=False):

    updated = False
    # so we could check and remove it to keep it clean
    temp_full_filename = None

    if url_filestamps is None:
        url_filestamps = {}

    url_filename = os.path.basename(urlsplit(href).path)

    class ReturnSooner(Exception):
        pass

    try: # might RF -- this is just to not repeat the same return
        if dry_run:
            # we can only try to deduce from the href...
            filename = url_filename
            full_filename = os.path.join(incoming, filename)
            # and not really do much
            lgr.debug("Nothing else could be done for download in dry mode")
            raise ReturnSooner

        # http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way
        request = urllib2.Request(href)

        # No traffic compression since we do not know how to identify
        # exactly either it has to be decompressed
        # request.add_header('Accept-encoding', 'gzip,deflate')
        r = urllib2.urlopen(request)
        try:
            r_info = r.info()

            r_filestamp = get_response_filestamp(r_info)
            filename = get_response_filename(href, r_info) or url_filename
            full_filename = os.path.join(incoming, filename)
            if r_filestamp['size']:
                lgr.debug("File %s is of size %d" % (filename, r_filestamp['size']))

            if url_filename != filename:
                lgr.debug("Filename in url %s differs from the load %s" % (url_filename, filename))

            # So we have filename -- time to figure out either we need to re-download it

            # url_filestamps might maintain information even if file is not present, e.g.
            # if originally we haven't kept originals
            download = False

            def _compare_stamps(ofs, nfs, msg):
                """ofs -- old filestamps, nfs -- new ones
                """
                download = False
                for k in ofs.keys():
                    n, o = nfs[k], ofs[k]
                    if n:
                        if o != n:
                            lgr.debug("Response %s %s differs from %s %s -- download"
                                      % (k, n, msg, o))
                            download = True
                return download

            if full_filename in url_filestamps:
                # if it is listed and has no stamps?
                # no -- it should have stamps -- we will place them there if none
                # was provided in HTML response. and we will redownload only
                # if any of those returned in HTTP header non-None and differ.
                # Otherwise we would assume that we have it
                # TODO: think if it is not too fragile
                download |= _compare_stamps(url_filestamps.get(full_filename),
                                            r_filestamp, "previous")

            if os.path.exists(full_filename):
                lgr.debug("File %s already exists under %s" % (filename, full_filename))

                # Verify time stamps etc
                file_stat = os.stat(full_filename)
                ex_filestamp = dict(size=file_stat.st_size,
                                    mtime=file_stat.st_mtime)

                download |= _compare_stamps(ex_filestamp, r_filestamp, "file stats")

            if not (full_filename in url_filestamps) and not os.path.exists(full_filename):
                lgr.debug("File %s is not known and doesn't exist" % filename)
                download = True

            if not download:
                raise ReturnSooner

            lgr.info("Need to download file %s into %s" % (filename, full_filename))

            if os.path.exists(full_filename):
                lgr.debug("Removing previously existing file")
                # TODO

            # actual download -- quite plain -- may be worth to offload to
            # wget or curl for now?
            temp_full_filename = full_filename + '.download'

            if os.path.exists(temp_full_filename):
                raise RuntimeError("File %s should not be there yet" % temp_full_filename)

            try:
                # we might need the directory
                full_filename_dir = os.path.dirname(temp_full_filename)
                if not os.path.exists(full_filename_dir):
                    os.makedirs(full_filename_dir)

                with open(temp_full_filename, 'wb') as f:
                    # No magical decompression for now
                    if False: #r.info().get('Content-Encoding') == 'gzip':
                        buf = StringIO( r.read())
                        src = gzip.GzipFile(fileobj=buf)
                    else:
                        src = r
                    shutil.copyfileobj(src, f)
            except Exception, e:
                lgr.error("Failed to download: %s" % e)
                if os.path.exists(temp_full_filename):
                    lgr.info("Removing %s" % temp_full_filename)
                    os.unlink(temp_full_filename)
                raise

            mtime = r_filestamp['mtime']
            size = r_filestamp['size']

            if mtime:
                lgr.debug("Setting downloaded file's mtime to %s obtained from HTTP header"
                          % r_filestamp['mtime'])
                os.utime(temp_full_filename, (time.time(), mtime))

            # Get stats and check on success
            # TODO: may be some would have MD5SUMS associated?
            updated = True

            # get mtime so we could update entry for our file
            file_stat = os.stat(temp_full_filename)
            new_mtime = file_stat.st_mtime
            new_size = file_stat.st_size

            if mtime and (new_mtime != mtime):
                lgr.debug("Set mtime differs for some reason.  Got %s (%s) while it should have been %s (%s)"
                          % (new_mtime, time.gmtime(new_mtime),
                             mtime, time.gmtime(mtime)))
                updated = False

            if size and (new_size != size):
                lgr.debug("Downloaded file differs in size.  Got %d while it should have been %d"
                          % (new_size, size))
                updated = False

            if updated:
                # TODO: we might like to git annex drop previously existed file etc
                os.rename(temp_full_filename, full_filename)

                url_filestamps[full_filename] = dict(mtime=mtime, size=size)
            else:
                pass
        finally:
            r.close()
            if temp_full_filename and os.path.exists(temp_full_filename):
                lgr.debug("Removing left-over %s of size %d"
                          % (temp_full_filename, os.stat(temp_full_filename).st_size))
                os.unlink(temp_full_filename)

    except ReturnSooner:
        # We have handled things already, just need to return
        pass

    return filename, full_filename, updated


def annex_init(path, description="", dry_run=False):
    cmd = "cd %s && git init && git annex init" % path

    lgr.info(dry("Initializing git annex repository under %s: %s"
            % (path, description), dry_run))

    if not dry_run:
        status, output = getstatusoutput(cmd, dry_run)
        lgr.debug("Successfully initialized")
        # dump description
        with open(os.path.join(path, '.git', 'description'), 'w') as f:
            f.write(description + '\n')


def annex_add(href, incoming_filename, annex_filename,
              uncomp_strip_leading_dir=True, #  False; would strip only if 1 directory
              addurl_opts=None, dry_run=False):

    lgr.info("Annexing %s originating from url=%s present locally under %s"
             % (annex_filename, href, incoming_filename))

    # Figure out if anything needs to be done to it
    annex_cmd = "addurl --file %s %s" \
        % (os.path.basename(annex_filename), href)

    if incoming_filename == annex_filename:
        # I guess not!
        pass
    else:
        # it might be that we would like to move it
        # or extract it
        is_directory = annex_filename.endswith('/')
        if is_directory:
            # figure out output directory name
            #annex_dir = os.path.dirname(annex_filename)+"TODO"
            annex_dir = annex_filename.rstrip('/')+"_TODO"
            annex_filename = annex_dir

            #import pydb; pydb.debugger()
            # what if the directory exist already?
            # option? yeah -- for now we will just REMOVE and recreate with new files
            temp_annex_dir = annex_dir + ".extract"
            os.makedirs(temp_annex_dir)
            decompress_file(incoming_filename, temp_annex_dir)
            if os.path.exists(annex_dir):
                lgr.debug("Removing previously present %s" % annex_dir)
                shutil.rmtree(annex_dir)
            os.rename(temp_annex_dir, annex_dir)
            annex_cmd = "add %s" % (os.path.basename(annex_filename),)
        else:
            pass
            #annex_cmd = "addurl --file %s %s" \
            #  % (os.path.basename(annex_filename), href)

    # So let's add file/or directory to annex? ;-)
    # TODO: --fast mode for arjlover
    cmd = "cd %s && git annex %s" \
      % (os.path.dirname(annex_filename), annex_cmd)

    status, output = getstatusoutput(cmd, dry_run)


def git_commit(path, msg=None, dry_run=False):
    if msg is None:
        msg = 'Committing changes as of %s' % time.strftime('%Y/%m/%d %H:%M:%S')
    cmd = "cd %s; git commit -m %r" % (path, msg)
    status, output = getstatusoutput(cmd, dry_run)

def pprint_indent(l, indent="", fmt='%s'):
    return indent + ('\n%s' % indent).join([fmt % x for x in l])

# Some rudimentary tests
from nose.tools import assert_equal, assert_raises

def test_filter_urls():
    urls = [('/x.nii.gz', 'bogus'),
            ('x.tar.gz', None),
            ('y', None)]
    assert_equal(filter_urls(urls, "^x\..*"), [urls[1]])
    assert_equal(filter_urls(urls, "^[xy]"), urls[1:])
    assert_equal(filter_urls(urls, "\.gz", "\.nii"),
           [urls[1]])
    assert_equal(filter_urls(urls, exclude_href="x"),
           [urls[2]])
    assert_equal(filter_urls(urls, "^[xy]"), urls[1:])

def test_get_response_filestamp():
    r = get_response_filestamp({'Content-length': '101',
                                'Last-modified': 'Wed, 01 May 2013 03:02:00 GMT'})
    assert_equal(r['size'], 101)
    assert_equal(r['mtime'], 1367377320)


def test_download_url():
    # let's simulate the whole scenario
    import tempfile
    fd, fname = tempfile.mkstemp(prefix='tmp-page2annex')
    dout = fname + '-d'
    os.mkdir(dout)
    os.write(fd, "How do I know what to say?\n")
    os.close(fd)

    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    assert(updated)
    # check if stats are the same
    s, s_ = os.stat(fname), os.stat(full_filename)
    assert_equal(s.st_size, s_.st_size)
    # at least to a second
    assert_equal(int(s.st_mtime), int(s_.st_mtime))

    # and if again -- should not be updated
    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    assert(not updated)

    # but it should if we remove it
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout)
    assert(updated)
    # check if stats are the same
    s_ = os.stat(full_filename)
    assert_equal(s.st_size, s_.st_size)
    assert_equal(int(s.st_mtime), int(s_.st_mtime))

    # and what if we maintain url_filestamps
    url_filestamps = {}
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout, url_filestamps)
    assert(updated)
    assert(full_filename in url_filestamps)
    s_ = os.stat(full_filename)
    assert_equal(int(s.st_mtime), int(s_.st_mtime))
    assert_equal(int(s.st_mtime), url_filestamps[full_filename]['mtime'])
    print filename, full_filename, url_filestamps,

    # and if we remove it but maintain information that we think it
    # exists -- we should skip it ATM
    os.unlink(full_filename)
    filename, full_filename, updated = download_url("file://%s" % fname, dout, url_filestamps)
    assert(not updated)
    assert(full_filename in url_filestamps)
    assert_raises(OSError, os.stat, full_filename)

    os.unlink(fname)
    shutil.rmtree(dout, True)

if __name__ == '__main__':

    # setup cmdline args parser
    # main parser
    import argparse
    import logging
    import os
    import sys

    lgr = logging.getLogger('page2annex')

    parser = argparse.ArgumentParser(
        fromfile_prefix_chars='@',
        description="""
    Fetch web page's linked content into git-annex repository.

    """,
        epilog='',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        add_help=True,
        version=__version__
    )

    # common options
    parser.add_argument(
        "-a", "--addurl-opts", default="",
        help="Additional options to pass to 'git annex addurl', e.g. --fast")
#    parser.add_argument(
#        "-i", "--include", type=RegexpType(),
#        help="Include links which match regular expression (by HREF). "
#             "Otherwise all are considered")
#    parser.add_argument(
#        "-e", "--exclude", type=RegexpType(),
#        help="Exclude links which match regular expression (by HREF)")
#     parser.add_argument(
#         "--use-a", action="store_true",
#         help="Use name provided in the link, not filename given by the server")
    parser.add_argument(
        "-n", "--dry-run", action="store_true",
        help="No git-annex is invoked, commands are only printed to the screen")

    if __debug__:
        # borrowed from Michael's bigmess
        parser.add_argument(
            '--dbg', action='store_true', dest='common_debug',
            help="do not catch exceptions and show exception traceback")

    parser.add_argument(
        '-l', '--log-level',
        choices=('critical', 'error', 'warning', 'info', 'debug'),
        default='warning',
        help="""level of verbosity""")

    parser.add_argument("config", metavar='file', nargs='+',
                        help="Configuration file(s) defining the structure of the 'project'")

    args = parser.parse_args() #['-n', '-l', 'debug', 'allen-genetic.cfg'])

    # Basic logging setup
    console = logging.StreamHandler(sys.stdout)
    console.setFormatter(logging.Formatter("%(levelname)-6s %(message)s"))
    lgr.addHandler(console)
    lgr.setLevel(getattr(logging, args.log_level.upper()))

    lgr.debug("Command line arguments: %r" % args)

    lgr.info("Running tests")
    test_filter_urls()
    test_get_response_filestamp()
    test_download_url()

    # Let's output summary stats at the end
    stats = dict([(k, 0) for k in ['sections', 'urls', 'downloads', 'size']])

    try:

        lgr.info("Reading configs")
        # Load configuration
        cfg = ConfigParser(defaults=dict(
            keep_orig='True',
            meta_info='True',
            directory="%(__name__)s",
            extract='\.(zip|tar\.gz)$',
            incoming="repos/incoming/EXAMPLE",
            public="repos/public/EXAMPLE",
            include_href='',
            include_href_a='',
            exclude_href='',
            exclude_href_a='',
            filename='&(filename)s',
            limit='0',                     # no limits
            annex='.*',                    # 
            ))
        cfg_read = cfg.read(args.config)
        assert(cfg_read == args.config)

        dry_str = "DRY: " if args.dry_run else ""

        incoming = cfg.get('DEFAULT', 'incoming')
        public = cfg.get('DEFAULT', 'public')

        if not (os.path.exists(incoming) and os.path.exists(public)):
            lgr.debug("%sCreating directories for incoming (%s) and public (%s) annexes"
                      % (dry_str, incoming, public))

            if not args.dry_run:
                if not os.path.exists(incoming):
                    os.makedirs(incoming)
                if not os.path.exists(public):
                    os.makedirs(public)           #TODO might be the same

        if not os.path.exists(os.path.join(public, '.git', 'annex')):
            annex_init(public, cfg.get('DEFAULT', 'description'), args.dry_run)

        # TODO: look what is in incoming for this "repository", so if
        # some urls are gone or changed so previous file is not there
        # we would clean-up upon exit

        # each section defines a separate download setup
        for section in cfg.sections():
            lgr.info("Section: %s" % section)
            stats['sections'] += 1

            section_dir = cfg.get(section, 'directory')

            incoming_section = os.path.join(incoming, section_dir)
            public_section = os.path.join(public, section_dir)

            if not (os.path.exists(incoming) and os.path.exists(public)):
                lgr.debug("%sCreating directories for section's incoming (%s) and public (%s) annexes"
                          % (dry_str, incoming_section, public_section))
                if not args.dry_run:
                    os.makedirs(incoming_section)
                    os.makedirs(public_section)           #TODO might be the same

            # TODO: load previous status info
            """We need

            url_filestamps -- to track their time.  URLs might or might not provide Last-Modified,
              so if not provided, would correspond to None and only look by url change pretty much

            annex_pairs -- to have clear correspondence between annex_filename and url.
                           annex_filename might correspond to a directory where we would
                           extract things, so we can't just geturl on it
            """
            status_info = dict(url_filestamps={},   # url -> (mtime, size (AKA Content-Length, os.stat().st_size ))
                               annex_pairs={})      # annex_filename -> url

            url_filestamps = status_info['url_filestamps']
            annex_pairs = status_info['annex_pairs']

            scfg = dict(cfg.items(section))
            page = fetch_page(scfg['url'])

            #
            # Parse out all URLs, as a tuple (url, a(text))
            urls_all = parse_urls(page)
            lgr.info("Got total %d urls" % len(urls_all))
            #lgr.debug("%d urls:\n%s" % (len(urls_all), pprint_indent(urls_all, "    ", "[%s](%s)")))

            # Filter them out
            urls = filter_urls(urls_all, **dict(
                [(k,scfg[k]) for k in
                 ('include_href', 'exclude_href', 'include_href_a', 'exclude_href_a')]))
            lgr.info("%d out of %d urls survived filtering" % (len(urls), len(urls_all)))
            if len(set(urls)) < len(urls):
                urls = sorted(set(urls))
                lgr.info("%d unique urls" % (len(urls),))
            lgr.debug("%d urls:\n%s" % (len(urls), pprint_indent(urls, "    ", "[%s](%s)"))) 
            if scfg['limit']:
                limit = int(scfg['limit'])
                if limit:
                    if len(urls) > limit:
                        raise RuntimeError("Cannot process section since we expected only %d urls"
                                           % limit)

            #
            # Process urls
            for href, href_a in urls:
                # bring them into the full urls
                href = urljoin(scfg['url'], href)
                lgr.debug("Working on [%s](%s)" % (href, href_a))

                # Will adjust url_filestamps in-place
                filename, full_filename, href_updated = \
                  download_url(href, incoming_section, dry_run=args.dry_run)

                if href_updated:
                    stats['downloads'] += 1
                    stats['size'] += os.stat(full_filename).st_size
                    # TODO: save bloody status db
                    pass

                # figure out what should it be
                annex_filename = scfg['filename'].replace('&', '%') % locals()   # interpolate
                annex_full_filename = os.path.join(public_section, annex_filename)

                if href_updated or (not annex_filename in annex_pairs):
                    # Place them under git-annex, if they do not exist already
                    annex_add(
                        href,
                        incoming_filename=full_filename,
                        annex_filename=annex_full_filename,
                        addurl_opts=args.addurl_opts,
                        dry_run=args.dry_run)

                    annex_pairs[annex_filename] = href
                else:
                    # TODO: shouldn't we actually check???
                    lgr.debug("Skipping annexing %s since it must be there already (TODO)" % annex_filename)

                # TODO: update status_info
                # TODO: save status_info
                stats['urls'] += 1

        git_commit(incoming)
        if incoming != public:
            git_commit(public)

    except Exception as exc:
        lgr.error('%s (%s)' % (str(exc), exc.__class__.__name__))
        if __debug__ and args.common_debug:
            import pdb
            pdb.post_mortem()
        raise
    lgr.info("Processed %(sections)d sections, %(urls)d urls, %(downloads)d downloads with %(size)d bytes" % stats)

